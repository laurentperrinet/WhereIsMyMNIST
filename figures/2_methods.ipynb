{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a classifier\n",
    "\n",
    "On commence par la fonction de base apprise de la librairie torch, cf https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:35.292747Z",
     "start_time": "2018-02-16T19:37:34.694408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300039\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.213460\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.170403\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.076578\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.867874\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.413523\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.000300\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.776155\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.459784\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.485992\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.438883\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.408428\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.460157\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.428865\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.398983\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.384978\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.298446\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.503128\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.523736\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.338001\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.367241\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.450433\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.304166\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.358748\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.331600\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.440495\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.363655\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.316201\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.201217\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.498405\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.324450\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.119690\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.190454\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.140863\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.314450\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.150085\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.289970\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.467413\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.214819\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.151107\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.225028\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.262922\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.233813\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.261397\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.212279\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.132899\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.279009\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.094898\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.127887\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.244892\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.339073\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.152845\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.090316\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.144601\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.198844\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.219203\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.062690\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.136392\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.115174\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.234771\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.063322\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.106191\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.160446\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.107596\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.177709\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.229862\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.073350\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.155467\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.277984\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.143156\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.117399\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.120456\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.076702\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.193632\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.069844\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.209615\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.137831\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.094175\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.107353\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.118349\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.145043\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.068249\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.023745\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.261361\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.091971\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.128656\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.190795\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.034261\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.035171\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.076297\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.118035\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.192009\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.204745\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.064555\n",
      "\n",
      "Test set: Average loss: 0.1022, Accuracy: 9663/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run what.py --epochs 1 --save-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On apprend une matrice de poids qui est fixée dans la suite et que nous allons utiliser pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "default = dict(batch_size=64, test_batch_size=1000, epochs=1, no_cuda=True, seed=42, lr=0.01, momentum=.5, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser.parse_args?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: batch_size test_batch_size epochs no_cuda seed lr momentum save_model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a5a61a314f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lr'"
     ]
    }
   ],
   "source": [
    "args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from what import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e1a39b33c8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[0;31m# =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1770\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSUPPRESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;31m# add any parser defaults that aren't present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(namespace=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from what import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N]\n",
      "                             [--epochs N] [--lr LR] [--momentum M] [--no-cuda]\n",
      "                             [--seed S] [--log-interval N] [--save-model]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/laurentperrinet/Library/Jupyter/runtime/kernel-696ed243-e233-4fc1-b07c-c71154e6e4cb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -ltr ../data/mnist_cnn.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:35.317080Z",
     "start_time": "2018-02-16T19:37:35.297625Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../data/mnist_cnn.pt\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading')\n",
    "    model.load_state_dict(torch.load(path))\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "    print('Done in ', time.time() - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:37.890051Z",
     "start_time": "2018-02-16T19:37:35.319232Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier in a standalone class\n",
    "\n",
    "Maintenant qu'on a appris les points qui permet une classification d'à peu près 98 % on va utiliser le modèle fead-forward pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.236066Z",
     "start_time": "2018-02-16T19:37:37.902628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ParvoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParvoNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "import torch\n",
    "test_batch_size = 1000\n",
    "test_batch_size = 20\n",
    "cmin, cmax = 0.1307, 0.3081\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('/tmp/data', train=False, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((cmin,), (cmax,))\n",
    "               ])), batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "def test(test_loader=test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "model = ParvoNet()\n",
    "path = \"MNIST_classifier.pt\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting the input images\n",
    "\n",
    "\n",
    "Je vais maintenant générer des données en utilisant les données originales de MNIST translatées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.247845Z",
     "start_time": "2018-02-16T19:37:42.240563Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "i_shift, j_shift = 12, 17\n",
    "N_pix = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:42.835649Z",
     "start_time": "2018-02-16T19:37:42.250490Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for data, target in test_loader:\n",
    "    break\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.225992Z",
     "start_time": "2018-02-16T19:37:42.839053Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
    "print(data_translate.shape)\n",
    "data_translate[:, :, (N_pix-i_shift):(2*N_pix-i_shift), (N_pix-j_shift):(2*N_pix-j_shift)] = data\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_translate[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.539188Z",
     "start_time": "2018-02-16T19:37:43.228276Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.550455Z",
     "start_time": "2018-02-16T19:37:43.543831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:43.960102Z",
     "start_time": "2018-02-16T19:37:43.552774Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_data(data, i_shift, j_shift):\n",
    "    N_pix = data.shape[-1]\n",
    "    assert(N_pix == data.shape[-2])\n",
    "    import numpy as np\n",
    "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 1, N_pix*3 - 1))\n",
    "    data_translate[:, :, (N_pix+i_shift):(2*N_pix+i_shift), (N_pix+j_shift):(2*N_pix+j_shift)] = data\n",
    "    data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "    return data_cropped\n",
    "\n",
    "data_cropped = shift_data(data, i_shift = 12, j_shift = -12)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier on the shifted data\n",
    "\n",
    "On peut maintenant tester le classifieur sur les images Translatées en calculant la valeur de classification en  fonction de l'erreur de localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:44.165284Z",
     "start_time": "2018-02-16T19:37:43.964216Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_shift(test_loader, i_shift, j_shift, verbose=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data_cropped = shift_data(data, i_shift=i_shift, j_shift=j_shift)        \n",
    "        data_cropped = torch.FloatTensor(data_cropped) #transforms.ToTensor()(data_cropped)\n",
    "        data_cropped, target = Variable(data_cropped, volatile=True), Variable(target)\n",
    "        output = model(data_cropped)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if verbose: print('\\nTest set: at ({}, {}), the  average loss is {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        i_shift, j_shift, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "path = \"MNIST_accuracy.npy\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy')\n",
    "    accuracy = np.load(path)\n",
    "else:\n",
    "    print('Computing accuracy')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    accuracy = np.zeros((2*N_pix-1, 2*N_pix-1))\n",
    "    from tqdm import tqdm\n",
    "    N_step = 1\n",
    "\n",
    "    with tqdm(total=(2*N_pix-1)**2/N_step**2) as pbar:\n",
    "        for i_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "            for j_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "                accuracy[i_shift+N_pix-1, j_shift+N_pix-1] = test_shift(test_loader, i_shift, j_shift)\n",
    "                pbar.update()\n",
    "    np.save(path, accuracy)\n",
    "    print('Done in ', time.time() - t0, 'seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'avoue que c'est un peu bourrin de calculer la classification sur les 128 × 128 pixels pour 1000 batch multiplié par 10 type d'entrées.... Mais bon on doit faire ça seulement une fois :-) (et sur CPU une classif = environ 300µs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:44:02.460123Z",
     "start_time": "2018-02-16T19:44:02.456249Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plt.pcolor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:53:37.955718Z",
     "start_time": "2018-02-16T19:53:37.258057Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "cmap = ax.pcolor(np.arange(-N_pix+1, N_pix+1), np.arange(-N_pix+1, N_pix+1), accuracy)\n",
    "ax.axis('equal')\n",
    "fig.colorbar(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T19:37:44.561298Z",
     "start_time": "2018-02-16T19:37:44.551578Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction de performance du classifieur  est calculée indépendamment de la forme spécifique du chiffre entre 0 et 9. Elle donne donc la carte de performance qu'on attend Au niveau de la classification/ On va pouvoir maintenant l'utiliser ceomm label pour apprendre de façon supervisée la correspondance entre la carte log-polaire obtenue depuis l'image brute et cette carte de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retinotopic mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation invariant power encoding (colliculus??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus = (retina_transform**2).sum(axis=(0, 3))\n",
    "#colliculus = colliculus**.5\n",
    "colliculus /= colliculus.sum(axis=-1)[:, :, None]\n",
    "print(colliculus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_vector = colliculus.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "print(colliculus_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colliculus_inverse = np.linalg.pinv(colliculus_vector)\n",
    "print(colliculus_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = (retina**2).sum(axis=(0,3)) \n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_azimuth*N_eccentricity, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "FIG_WIDTH = 5 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_WIDTH))\n",
    "for i_orient in range(N_azimuth):\n",
    "    for i_scale in range(N_eccentricity):\n",
    "        env = np.sqrt(energy[i_orient, i_scale, :]**2.5).reshape((N_X, N_Y))\n",
    "        ax.contour(energy[i_orient, i_scale, :].reshape((N_X, N_Y)), levels=[env.max()/2], lw=1,\n",
    "                  colors=[plt.cm.rainbow(i_scale * 1.5/N_azimuth)])\n",
    "fig.suptitle('Tiling of visual space using energy', y=1.02)\n",
    "ax.set_xlabel(r'$Y$')\n",
    "ax.set_ylabel(r'$X$')\n",
    "ax.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
